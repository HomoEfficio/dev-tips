# Kafka 둘러보기

# 디스크에 읽고 쓰는데도 성능이 좋은 이유

- 아래 문서 요약 및 양념
    - [아파치 카프카 문서 - 데이터 저장](https://kafka.apache.org/documentation/#persistence)
    - [아파치 카프카 문서 - 효율 극대화](https://kafka.apache.org/documentation/#maximizingefficiency)

## Linear Write/Read

일반적으로 디스크 접근은 메모리에 비해 매우 느리다고 알려져 있지만, 그건 랜덤 액세스일 때 얘기고 선형 접근(또는 순차 접근?)이라면 디스크의 속도도 상당히 쓸만하다. [때로는 메모리를 랜덤 액세스하는 것보다 요즘 나온 HDD에서의 순차 접근이 더 빠르다는 결과](https://queue.acm.org/detail.cfm?id=1563874)도 있다고 한다.

![Imgur](https://i.imgur.com/1GeBFO9.jpg)  
그림 출처: https://queue.acm.org/detail.cfm?id=1563874

또한 순차 접근은 예측성이 좋아서 운영체제에 의해 엄청나게 최적화된다. 요즘 운영체제는 대량의 블록에서 데이터를 미리 읽는 read-ahead와 작은 쓰기 작업을 모아서 한 번에 쓰는 write-behind를 지원하므로 순차 접근만 가능하다면 디스크를 사용해도 성능적으로 나쁠 것이 없다.

**카프카가 데이터를 디스크에 쓰고 읽으면서도 처리량이 뛰어난 이유 중의 하나는, 카프카의 주요 용도인 메시지 큐의 특성상 데이터를 읽고 쓰는 방식을 순차 접근으로 구성할 수 있기 때문이다.**

## Page Cache

순차 접근이 아닌 경우에는 디스크 접근은 속도가 매우 느리므로 운영체제는 디스크 캐시를 위해 메인 메모리를 공격적으로 사용하게 되었다. 모든 디스크 읽기/쓰기는 이 디스크 캐시를 거치게 되고, 이 기능을 강제로 끄는 것은 쉽지 않으므로, 애플리케이션에서 내부적으로 캐시를 사용한다면 실제로는 같은 내용을 운영체제의 페이지 캐시(페이지 단위의 디스크 캐시)와 애플리케이션 내부 캐시에 중복 저장하게 된다.

게다가 JVM은 객체의 메모리 오버해드가 상당히 크고, 힙 내 데이터가 많아질 수록 GC도 느려지고 성가신 일이 된다.

따라서 **카프카는 힙 메모리 내에 객체로 저장하는 대신 바이트 구조로 압축하고 운영체제가 훌륭하게 최적화하고 있는 페이지 캐시를 활용해서 디스크에 저장하는 방식을 도입해서 성능을 높일 수 있었다.** 미사용 메모리에 대한 automatic access와 바이트 구조로의 압축을 통해 32G 장비에서 28-30G를 페이지 캐시로 활용할 수 있었고 GC 부담 없이 성능을 높였다. 게다가 이 **페이지 캐시는 애플리케이션 수준이 아니라 운영체제 수준에서 관리되므로, 애플리케이션 서비스 재시작 시에도 별도의 워밍업 없이도 좋은 성능을 낼 수 있다.**

**파일시스템에 쓴다고는 하지만 실제로는 커널의 페이지 캐시로 데이터를 전송한다고 볼 수 있다.**

## 단순한 자료 구조

메시지 큐에 데이터를 저장하는 자료 구조로 BTree를 사용하기도 하는데, BTree가 O(logN)으로 탐색 효율이 매우 뛰어나지만 데이터가 저장되는 곳이 메모리가 아니라 디스크라면 얘기가 달라진다.

**카프카는 BTree를 사용하지 않고 그냥 파일 끝에 데이터를 추가하는 단순한 자료 구조를 사용**한다. 이게 가능한 이유도 앞에서 살펴본 것처럼 디스크 사용 방식이 대부분 순차 접근이기 때문이다. 따라서 **그냥 파일 끝에 추가하므로 데이터 양이 늘어난다고 성능이 낮아질 일도 없으므로 사실 상 O(1)이라고 할 수 있고, 읽기 작업이 쓰기 작업을 블록킹 할 일도 없다.**

따라서 **굳이 고성능의 디스크를 사용할 필요가 없으므로 같은 값으로 용량이 큰 디스크를 사용할 수 있고, 이를 통해 확보한 공간에 메시지를 장기간(기본은 일주일) 저장할 수 있으므로 다른 메시징 시스템과 차별화된 기능을 제공할 수 있게 된다.**

![https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines](https://i.imgur.com/SUFqyUe.png)  
Throughput 그림 - write 데이터가 누적되어 증가해도 초당 처리량(레코드 수)는 큰 변화 없다.  
그림 출처: https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines

## 일괄 처리

메시지를 쓰거나 읽을 때 개별 건마다 처리하지 않고 여러 건을 묶어서 일괄(batch) 처리하는 단순한 최적화를 통해 막대한 성능 개선 효과를 볼 수 있다. **일괄 처리는 한 번 전송에 사용되는 네트워크 패킷의 크기를 확대하므로 네트워크 I/O 오버헤드르 줄이고, 대규모의 순차 디스크 작업으로 이어지며, 연속적인 메모리 블록 사용으로 이어지는 등 효율 개선 효과가 매우 크다.** 

## Zero Copy

일반적인 데이터 전송은 다음과 같은 흐름으로 수행된다.

1. 운영체제가 디스크의 데이터를 커널 공간의 페이지 캐시로 읽어들인다.
1. 애플리케이션은 커널 공간의 데이터를 사용자 공간의 버퍼로 읽어들인다.
1. 애플리케이션은 사용자 공간의 버퍼에 있는 데이터를 커널 공간에 쓰고 이어서 소켓 버퍼에 쓴다.
1. 운영체제는 소켓 버퍼에 있는 데이터를 복사해서 NIC(Network Interface Card) 버퍼에 쓴다.

이 방식에는 네 번의 복사와 두 번의 시스템 콜이 동원된다. 하지만 **[`sendfile`](http://man7.org/linux/man-pages/man2/sendfile.2.html)을 사용하면 운영체제가 페이지 캐시에 있는 데이터를 NIC 버퍼에 직접 쓸 수 있으므로 단 한 번의 복사 작업으로 데이터를 네트워크에 전송할 수 있다.**

페이지 캐시와 zero copy의 조합을 통해 카프카 클러스터에서는 데이터가 페이지 캐시에서 직접 NIC 버퍼로 직접 전송되므로 디스크 읽기가 전혀 발생하지 않기도 한다.

Java의 zero copy는 [IBM 문서](https://developer.ibm.com/articles/j-zerocopy/) 를 참고한다.

## 종단간 메시지 일괄 압축

메시지를 WAN(Wide Area N/W)에 걸쳐 있는 데이터 센터에 전송해야 하는 데이터 파이프라인에서는 병목이 CPU나 디스크가 아니라 네트워크 대역폭에서 발생하기도 한다. 

사용자 쪽에서의 압축을 통해 일부분 효과를 볼 수 있지만, JSON의 필드명이나 웹 로그의 user agent 등 동일한 타입의 데이터를 담은 메시지에서 발생하는 중복이 압축 효율을 떨어뜨리는 주요인 중 하나이므로 사용자 쪽에서 개별 메시지에 대해 건건이 압축을 하는 것보다 카프카에서 묶어서 일괄 압축하는 것이 효율이 훨씬 좋다.

카프카는 GZIP, Snappy, LZ4, ZStandard 방식으로 메시지 일괄 압축을 지원하며, 이렇게 일괄 압축한 메시지는 그대로 디스크에 쓰고 그대로 컨슈머에게 전송되며, 컨슈머가 압축을 해제하고 데이터를 사용하게 된다.


# 등장 인물

- 브로커: Kafka가 설치/실행되는 노드
- 컨트롤러: Kafka 클러스터에서 장애처리 등을 담당하는 특수 브로커로 하나만 존재
- 주키퍼: Kafka 클러스터 구성 및 관리(구버전에서는 메시지의 offset 등 메타데이터도 관리했으나 0.9 이후로는 브로커에서 관리)
- 프로듀서: 데이터 생산자로서 카프카에게 데이터를 보내는 주체. 실제 구현은 카프카가 제공하는 `KafkaProducer`, `ProducerRecord` 클래스를 사용.
- 컨슈머: 데이터 소비자로서 카프카로부터 데이터를 읽는 주체. 실제 구현은 카프카가 제공하는 `KafkaConsumer`, `ConsumerRecord` 클래스를 사용.
- 리더: 토픽의 파티션 기준으로 원본을 가진 노드. 팔로워의 정상 동작 여부를 지속적으로 체크. 데이터 읽기/쓰기 담당
- 팔로워: 토픽의 파티션 기준으로 사본을 가진 노드. 리더에게서 사본을 가져와 관리. 리더에 장애가 나면 팔로워 중 하나가 리더가 된다.


# 데이터 모델

## 컨수머 그룹

- 동일한 토픽에 대해 여러 컨수머가 메시지를 가져갈 수 있도록 컨수머를 묶어주는 단위
- 컨수머 그룹 별로 오프셋이 따로 관리되므로, 여러 개의 컨수머 그룹이 하나의 토픽에서 안정적으로 메시지를 가져갈 수 있음

## 토픽

- 전송된 메시지를 분류할 수 있는 단위로서 프로듀서는 토픽 별로 메시지를 전송
- 토픽은 하나 이상의 파티션으로 나눠서 사용 가능
- 프로듀서는 메시지 전송 시 토픽만 지정 가능하므로, 토픽의 파티션이 2개 이상인 경우 해당 토픽에 전송된 메시지가 어느 파티션에 저장될지는 카프카가 결정(어떤 기준?)

## 파티션

- 하나의 토픽을 여러 파티션으로 나눠서 병렬 송신/수신 가능
- 각 파티션은 브로커의 디렉터리로 매핑. 디렉터리마다 인덱스 파일, 데이터 파일 2개의 파일 핸들 점유
- 리플리케이션도 파티션 단위로 동작하며 하나는 파티션 리더, 나머지는 파티션 팔로워
- 파티션 별 리더 선출은 직렬로 수행되므로 리더 선출 소요 시간은 파티션 수에 비례하여 증가
- 파티션 증가는 무중단으로도 적용할 수 있지만 파티션 감소는 아예 불가능
- 하나의 파티션에 있는 메시지는 하나의 컨수머에서만 읽어갈 수 있게 해서 메시지 처리 순서 보장
- 파티션 내의 메시지는 순서가 보장되지만, 토픽 내 파티션 사이의 메시지 순서는 보장되지 않는다.
  - 실제로는 메시지 a2가 먼저 토픽의 파티션2에 저장됐지만 나중에 파티션0에 저장된 메시지 a3가 a2보다 먼저 consume 될 수도 있다.
  - 왜냐하면 파티션 별로 컨수머가 다를 수 있고, 파티션2에서 메시지를 가져가는 컨수머가 더 빨리 consume 할 수 있기 때문

## 오프셋

- 파티션 내 유일하며 순차적으로 증가하는 64비트 정수
- 컨슈머는 오프셋으로 정해진 순서대로만 메시지를 가져갈 수 있음
- 컨수머 그룹 별로 따로 관리된다
  - 토픽0, 파티션0에 저장된 데이터를 컨수머그룹a의 컨수머 a1과 컨수머그룹b의 컨수머 b2가 읽어가게 돼 있을 때,
  - a1이 인식하는 토픽0, 파티션0의 오프셋과 b2가 인식하는 토픽0, 파티션0의 오프셋 값이 다르다.


# 고가용성

## 리더-팔로워

- 주키퍼와 카프카는 원본을 관리하는 주체를 리더, 사본을 관리하는 주체를 팔로워라고 부름
- 하나의 노드가 토픽1-파티션0의 리더이면서 토픽1-파티션1의 팔로워일 수 있다.
- 카프카의 모든 읽기/쓰기는 리더를 통해서만 가능
    - 팔로워는 리더의 데이터를 복제해서 보유하기만 하고 읽기/쓰기에는 관여하지 않음

## Replication

- 리플리케이션은 파티션 단위로 수행. 즉 파티션 별로 리더, 팔로워 존재
- 리플리케이션 팩터는 토픽 별로 설정
- 클러스터에서는 무중단 설정 변경 가능
    - config/server.properties 에 있는 리플리케이션 팩터 변경 후 해당 브로커만 재시작 -> 클러스터 내 전체 브로커에서 반복
- ISR(In Sync Replication)
    - 리플리케이션 그룹. 따라서 파티션 별로 존재
    - ISR 내에 있는 팔로워만 해당 파티션의 리더 후보가 될 수 있음
    - 리더는 팔로워의 정상 동작여부를 체크하며, 일정 시간 정상 동작하지 않으면 ISR에서 제외

# z-node

- 유닉스 파일시스템의 i-node 처럼 주키퍼에서 정보를 관리하는 단위
- 주키퍼 shell(zkCli.sh)에서 `ls` 로 확인 가능

# 계속...
